{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e407659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try not to become a man of success. Rather become a man of value. :  52.9447%\n",
      "It takes courage to grow up and become who you really are. :  39.7146%\n",
      "To love at all is to be vulnerable. Love anything and your heart will be wrung a :  10.7231%\n"
     ]
    }
   ],
   "source": [
    "#   Name: Andrew Cui, Remy Zhang, Haoyang Zhou\n",
    "# Course: CSCI 185\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# key = author , val = quotes written by the author\n",
    "quotes_by_author = {}\n",
    "numquotes = 0\n",
    "\n",
    "# key = quote, val = tags attached to the quote\n",
    "quotes_tag = {}\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # the i-th link\n",
    "    if i == 0:\n",
    "        url = 'https://quotes.toscrape.com'\n",
    "    else:\n",
    "        url = 'https://quotes.toscrape.com/page/' + str(i + 1) + '/'\n",
    "    \n",
    "    # open the website xml file\n",
    "    soup = BeautifulSoup(urlopen(url), 'html.parser')\n",
    "    \n",
    "    \n",
    "    # scrape all qoutes on that page\n",
    "    quotes = soup.find_all(\"div\", class_=\"quote\")\n",
    "    \n",
    "    \n",
    "    for quote in quotes:\n",
    "        \n",
    "        # for each quote, we do the following\n",
    "        \n",
    "        # keep track of total number of quotes\n",
    "        numquotes = numquotes + 1\n",
    "        \n",
    "        # the actual quote\n",
    "        text = quote.find(\"span\", class_=\"text\").text\n",
    "        text = text[1:len(text) - 1]\n",
    "        \n",
    "        # the author\n",
    "        author = quote.find(\"small\", class_=\"author\").text\n",
    "        \n",
    "        # save the quote according to its author\n",
    "        if author in quotes_by_author:\n",
    "            quotes_by_author[author].append(text)\n",
    "        else:\n",
    "            quotes_by_author[author] = [text]\n",
    "        \n",
    "        # find the tags\n",
    "        tags = []\n",
    "        _tags = quote.find(\"div\", class_=\"tags\")\n",
    "        \n",
    "        for _tag in _tags.find_all(\"a\", class_=\"tag\"):\n",
    "            tag = _tag.text\n",
    "            tags.append(tag)\n",
    "        \n",
    "        # save the tags to the corresponding quote\n",
    "        quotes_tag[text] = tags\n",
    "        \n",
    "        \n",
    "\n",
    "# find term frequency\n",
    "\n",
    "# tf (term frequency) is a 2-D dictionary whose first key is the quote\n",
    "# and the second key is a unique token in the quote\n",
    "tf = {}\n",
    "for author in quotes_by_author:\n",
    "    for quote in quotes_by_author[author]:\n",
    "        # for each quote:\n",
    "        \n",
    "        # a dictionary that stores the term frequency of all tokens in the current quote\n",
    "        # key = a unique token, val = count of the token\n",
    "        term_frequency = {}\n",
    "        \n",
    "        # counts the max occurance of any token\n",
    "        max = 0\n",
    "        \n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = []\n",
    "        \n",
    "        # filter stop words\n",
    "        for word in word_tokenize(quote):\n",
    "            if word not in stop_words:\n",
    "                tokens.append(word)\n",
    "        \n",
    "        # count the occurance of each token\n",
    "        for word in tokens:\n",
    "            if word in term_frequency:\n",
    "                term_frequency[word] = term_frequency[word] + 1\n",
    "                if max < term_frequency[word]:\n",
    "                    max = term_frequency[word]\n",
    "            else:\n",
    "                term_frequency[word] = 1\n",
    "                if max < 1:\n",
    "                    max = 1\n",
    "        \n",
    "        # normalize\n",
    "        for token in term_frequency:\n",
    "            term_frequency[token] = term_frequency[token] / max\n",
    "        \n",
    "        # save\n",
    "        tf[quote] = term_frequency\n",
    "\n",
    "        \n",
    "# term frequency by tags\n",
    "# since tages of a quote are unique, we use the quotes_tag dictionary to calculate tff\n",
    "\n",
    "\n",
    "# find document frequency\n",
    "\n",
    "df = {}\n",
    "for quote in tf:\n",
    "    for token in tf[quote]:\n",
    "        # for each unique in a quote, we increment the count\n",
    "        \n",
    "        if token in df:\n",
    "            df[token] = df[token] + 1\n",
    "        else:\n",
    "            df[token] = 1\n",
    "\n",
    "            \n",
    "# find document frequency by tags\n",
    "df_tag = {}\n",
    "for quote in tf_tag:\n",
    "    for tag in tf_tag[quote]:\n",
    "        if tag in df_tag:\n",
    "            df_tag[tag] = df_tag[tag] + 1\n",
    "        else:\n",
    "            df_tag[tag] = 1            \n",
    "\n",
    "            \n",
    "# tf_idf is a 2D dictionary whose keys are the quote and tokens in the quote\n",
    "# and the value is the tf-idf score\n",
    "tf_idf = {}\n",
    "\n",
    "# find tf-idf for each token\n",
    "for quote in tf:\n",
    "    tf_idf[quote] = {}\n",
    "    for token in tf[quote]:\n",
    "        # for each token in a quote, we calculate its tf-idf score\n",
    "        _tf = tf[quote][token]\n",
    "        _idf = math.log(numquotes / (df[token]))\n",
    "        tf_idf[quote][token] = _tf * _idf\n",
    "\n",
    "# tf-idf by tags\n",
    "tf_idf_tag = {}\n",
    "\n",
    "for quote in tf_tag:\n",
    "    tf_idf_tag[quote] = {}\n",
    "    for tag in tf_tag[quote]:\n",
    "        _tf = 0\n",
    "        if tag in quotes_tag[quote]:\n",
    "            _tf = 1\n",
    "        _idf = math.log(numquotes / (df_tag[tag]))\n",
    "        tf_idf_tag[quote][tag] = _tf * _idf\n",
    "\n",
    "# the search function by calculating the cosine similarity between tokens of each document and the query\n",
    "def search_by_tokens(query):\n",
    "\n",
    "    # tokenize query and rmeove stop words\n",
    "    words = []\n",
    "    for word in word_tokenize(query):\n",
    "        if word not in stop_words:\n",
    "            words.append(word)\n",
    "\n",
    "    max = 0\n",
    "    query_vector = {}\n",
    "\n",
    "    # count the term frequencies for the query and store in query_vector\n",
    "    for word in words:\n",
    "        if word in query_vector:\n",
    "            query_vector[word] = query_vector[word] + 1\n",
    "            if max < query_vector[word]:\n",
    "                max = query_vector[word]\n",
    "        else:\n",
    "            query_vector[word] = 1\n",
    "            if max == 0:\n",
    "                max = 1\n",
    "    \n",
    "    # a helper function to calculate idf\n",
    "    def idf(key):\n",
    "        if key in df:\n",
    "            return math.log(numquotes / (df[key]))\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    # translate tf vector to tf-idf vector and find the |q|, query norm\n",
    "    query_norm = 0\n",
    "    for key in query_vector:\n",
    "        \n",
    "        # we first normalize the query vector, and multiply with idf\n",
    "        query_vector[key] = query_vector[key] / max * idf(key)\n",
    "        \n",
    "        # calculate query_norm\n",
    "        query_norm = query_norm + query_vector[key] * query_vector[key]\n",
    "\n",
    "    query_norm = math.sqrt(query_norm)\n",
    "\n",
    "    # a dictionary whose key is the quotes and whose values are the cosine similarity between the quote and the query\n",
    "    similarity = {}\n",
    "\n",
    "    # find the cosine sim between each file and query\n",
    "    for file in tf:\n",
    "        norm = 0\n",
    "\n",
    "        dot_product = 0\n",
    "        \n",
    "        # grab all tokens in this quote\n",
    "        for token in tf[file]:\n",
    "            # calculate the norm of this quote\n",
    "            norm = norm + tf_idf[file][token] * tf_idf[file][token]\n",
    "            \n",
    "            # calculate the dot product of the quote and the query\n",
    "            if token in query_vector:\n",
    "                dot_product = dot_product + query_vector[token] * tf_idf[file][token]\n",
    "        norm = math.sqrt(norm)\n",
    "        \n",
    "        # if the dot product is 0, it means that the cosine similarity must be 0.\n",
    "        # so this quote would have nothing in common with the query\n",
    "        if dot_product == 0:\n",
    "            continue\n",
    "            \n",
    "        # calculate the cosine_sim\n",
    "        cosine_sim = dot_product / (norm * query_norm)\n",
    "        similarity[file] = cosine_sim\n",
    "    # end for\n",
    "\n",
    "    # sort and return\n",
    "    return sorted(similarity.items(), key=lambda x:x[1], reverse = 1)\n",
    "\n",
    "# the search function by calculating the cosine similarity between tags of each document and the query\n",
    "def search_by_tags(query):\n",
    "   \n",
    "    # tokenize query\n",
    "    words = []\n",
    "    for word in word_tokenize(query):\n",
    "        if word not in stop_words:\n",
    "            words.append(word)\n",
    "\n",
    "    # find the term frequencies for the query\n",
    "    max = 0\n",
    "    query_vector = {}\n",
    "    \n",
    "    # count the term frequencies for the query and store in query_vector\n",
    "    for word in words:\n",
    "        if word in query_vector:\n",
    "            query_vector[word] = query_vector[word] + 1\n",
    "            if max < query_vector[word]:\n",
    "                max = query_vector[word]\n",
    "        else:\n",
    "            query_vector[word] = 1\n",
    "            if max == 0:\n",
    "                max = 1\n",
    "                \n",
    "                \n",
    "    # a helper function to calculate idf    \n",
    "    def idf(key):\n",
    "        if key in df_tag:\n",
    "            return math.log(numquotes / (df_tag[key]))\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    # translate tf vector to tf-idf vector and find the |q|, query norm\n",
    "    query_norm = 0\n",
    "    for key in query_vector:\n",
    "        query_vector[key] = query_vector[key] / max * idf(key)\n",
    "        query_norm = query_norm + query_vector[key] * query_vector[key]\n",
    "\n",
    "    query_norm = math.sqrt(query_norm)\n",
    "\n",
    "\n",
    "    similarity = {}\n",
    "\n",
    "    # find the cosine sim between each file and query\n",
    "    for file in tf_tag:\n",
    "        norm = 0\n",
    "\n",
    "        dot_product = 0\n",
    "        \n",
    "        # grab all tags in this quote\n",
    "        for tag in tf_tag[file]:\n",
    "            # calculate the norm of the quote\n",
    "            norm = norm + tf_idf_tag[file][tag] * tf_idf_tag[file][tag]\n",
    "            \n",
    "            # calculate the dot product\n",
    "            if tag in query_vector:\n",
    "                dot_product = dot_product + query_vector[tag] * tf_idf_tag[file][tag]\n",
    "        norm = math.sqrt(norm)\n",
    "        \n",
    "        \n",
    "        # if dot prod == 0, the quote has nothing in commen with the query.\n",
    "        if dot_product == 0:\n",
    "            continue\n",
    "        cosine_sim = dot_product / (norm * query_norm)\n",
    "        \n",
    "        similarity[file] = cosine_sim\n",
    "\n",
    "    # sort and return\n",
    "    return sorted(similarity.items(), key=lambda x:x[1], reverse = 1)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "results = search_by_tokens(\"love and life\")\n",
    "count = 1\n",
    "for s in results:\n",
    "    count = count + 1\n",
    "    print(s[0][:80], ': ', str(s[1] * 100)[:7] + '%')\n",
    "    if count > 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e8ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
